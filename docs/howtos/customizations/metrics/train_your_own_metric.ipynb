{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff2c00f7",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "## Train your own metric\n",
    "\n",
    "LLM as judge metric often makes mistakes and lack alignment with human evaluators. This makes them risky to use as their results cannot be trusted fully. Now, you can fix this using ragas. This simple tutorial notebook showcasing how to train and align any LLM as judge metric using ragas. One can use this to train any LLM based metric in ragas. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d79c6c-5316-41ce-84d1-61a7a3d4a320",
   "metadata": {},
   "source": [
    "\n",
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f9dd63-91c7-4ba4-a21f-7dbb54ce2414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/ragas/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (None) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/ragas/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "from ragas import evaluate, EvaluationDataset\n",
    "from ragas.metrics import AspectCritic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d03eb0",
   "metadata": {},
   "source": [
    "Now, sign up for a free account at [app.ragas](https://app.ragas.io) and get your API key.\n",
    "Navigate to App tokens -> Create new token. Copy the key and paste it in the below code. Store it safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b4505-68f4-4a07-8f79-73da8d4ef3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['RAGAS_APP_TOKEN'] = 'your_app_token'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82272d-eb13-483c-ba06-db4c0a73f3fd",
   "metadata": {},
   "source": [
    "### Setup the models used for evaluation and training\n",
    "You may choose any LLM model for training and evaluation. Here's [how to do it](../customize_models.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2461866d-e8c6-4cbf-b1f2-20079069c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o-mini\"))\n",
    "embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebeb666",
   "metadata": {},
   "source": [
    "### Load sample evaluation dataset\n",
    "Here, we are loading the sample dataset for evaluation. You can replace it with your own dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9dcab3f-325a-4b55-a662-759db6a2a9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"explodinggradients/ELI5\",split=\"test\")\n",
    "eval_dataset = EvaluationDataset.from_hf_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a83029",
   "metadata": {},
   "source": [
    "### Setup the Metric\n",
    "You may use any LLM based metric. For simplicity, I am using aspect critic metric and setting it up so that it can compare the response with the reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43df046e-07de-4967-a232-54578fb3f880",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic = AspectCritic(name=\"answer_correctness\",definition=\"Given the user_input, reference and response. Is the response correct compared with the reference\",llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27caa5a5",
   "metadata": {},
   "source": [
    "### Evaluate and Upload the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6964abe-0360-4249-946a-5c388a166758",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.49it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(eval_dataset,metrics=[critic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea52de01-b639-4011-8def-6321b54c2bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/a6baf6ff-027f-4097-89e3-e11c70b8cf61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/a6baf6ff-027f-4097-89e3-e11c70b8cf61'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99767b85-08cd-45d7-b81b-5ab2c6aeffd3",
   "metadata": {},
   "source": [
    "### Review and annotate some results\n",
    "You may now view and annotate the evaluation results in app.ragas. These annotations will be used to train the metric. Please make sure to annotate at least 15-20 examples for good results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a2463-08f6-4639-b63f-2e27966afd12",
   "metadata": {},
   "source": [
    "### Train the metric\n",
    "Download the annotated samples from app.ragas using `Download annotated json` button. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5fb47d6f-4514-4dc3-89ca-0be074f58397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.config import InstructionConfig,DemonstrationConfig\n",
    "demo_config = DemonstrationConfig(embedding = embeddings)\n",
    "inst_config = InstructionConfig(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2df934d-5701-4c64-972f-2455d3d915d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feedback Mutation Step 2/4:  19%|███████▍                               | 28/146 [00:03<00:10, 11.23it/s]No samples found for the feedback generation.\n",
      "No feedbacks found for the prompt single_turn_aspect_critic_prompt. Returning the original prompt.\n",
      "Feedback Mutation Step 2/4:  23%|████████▊                              | 33/146 [00:06<00:26,  4.33it/s]No samples found for the feedback generation.\n",
      "No feedbacks found for the prompt single_turn_aspect_critic_prompt. Returning the original prompt.\n",
      "Feedback Mutation Step 2/4:  24%|█████████▎                             | 35/146 [00:06<00:22,  4.97it/s]Error in LangChainTracer.on_chain_end callback: TracerException('No indexed run ID 2046bdfe-27cc-4ce2-b999-3e8fc674969c.')\n",
      "Fitness Evaluation Step 4/4: 100%|█████████████████████████████████████| 146/146 [00:24<00:00,  6.03it/s]\n",
      "Few-shot examples [single_turn_aspect_critic_prompt]: 100%|██████████████| 18/18 [00:09<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "critic.train(path=\"edited_chain_runs.json\",demonstration_config=demo_config,instruction_config=inst_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9e387-e413-42c2-a188-bce106a7d526",
   "metadata": {},
   "source": [
    "### Inspect\n",
    "Now, let's do some analysis on the trained metric.\n",
    "\n",
    "First, let's take a look at new instructions that was obtained for the metric after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74574d75-dc1d-4319-99db-ea7ac284b098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Evaluate the provided user responses against the reference information for accuracy and completeness. Assign a verdict of 1 if the response is accurate and aligns well with the reference, or 0 if it contains inaccuracies or misrepresentations.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "critic.get_prompts()['single_turn_aspect_critic_prompt'].instruction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d710de9-9cb2-4517-a816-11d52e41c35e",
   "metadata": {},
   "source": [
    "#### Re-evaluate\n",
    "Let's evaluate again and see if the metric has improved for any un-annotated examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6beed95-63d6-4df5-afe4-09c4a1c5f1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████| 50/50 [00:28<00:00,  1.78it/s]\n"
     ]
    }
   ],
   "source": [
    "results = evaluate(eval_dataset,metrics=[critic])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d125209-fe42-4299-b878-fefc0d837247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results uploaded! View at https://app.ragas.io/dashboard/alignment/evaluation/687e7cdf-ff31-4c15-9780-c179207c929c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://app.ragas.io/dashboard/alignment/evaluation/687e7cdf-ff31-4c15-9780-c179207c929c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc991be",
   "metadata": {},
   "source": [
    "Here in my case, the metric has improved significantly. You can see the difference in the scores. To show the difference, let's compares the scores and changed reasoning for one specific example before and after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b041f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragas",
   "language": "python",
   "name": "ragas"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
